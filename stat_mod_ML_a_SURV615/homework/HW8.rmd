---
title: Homework 8
author: Kevin Linares and Jamila Sani
date: "`r format(Sys.time(), '%d %B, %Y')`"
embed-resources: true
editor: visual
execute:
  warning: false
  error: false
  tidy: true
output: pdf_document
theme: lux
---

<br> <br>

------------------------------------------------------------------------

```{r}
library(jtools)
library(faraway)
library(olsrr)
library(knitr)
library(sandwich)
library(corrplot)
library(tidyverse)

options(scipen=999)

#________________ handy functions __________________
kappa_fun <- function(mod_name){
  # model matrix w/o intercept
  x_mod = model.matrix(mod_name)[,-1] 
  #  matrix multiplication
  e=eigen(t(x_mod)%*%x_mod) 
  message("Inspect wide range in eigenvalues")
  print(e$val) 
  
  # calculate kappa values
  message("Inspect if Kappa conditional number value > 30")
  kappa_mod = sqrt(max(e$val)/min(e$val))
  print(kappa_mod)
  message("Inspect condition index, of at least one linear combination")
  nu = sqrt(max(e$val)/e$val)
  print(nu)
}



f_stat_lack_fit_fun <- function(mod_orig, mod_lack_fit){
  
  lack_fit_ss = anova(mod_orig)[[2]][2]
  pure_error = anova(mod_lack_fit)[[2]][2]
  # df lack of fit = # of unique x values - number of parameters
  df_lack_fit = length(unique(mod_orig$model$speed)) - 
    length(mod_cars$coefficients)
  # df pure error = # of obs - # of unique x values 
  df_pure_error = length(mod_orig$model$speed) - 
    length(unique(mod_orig$model$speed))
  
  f_stat = ((
    # difference in residuals between models
    (lack_fit_ss - pure_error) / 
      # df lack of fit - number number parameters
      df_lack_fit ) /
      # divide pure error by # of unique x values
      (pure_error / df_pure_error) 
  )
  
  # Calculate p-value for lack of fit test
  p_value = 1 - pf(f_stat, df1=df_lack_fit, df2=df_pure_error)
  
  message(str_c("The F statistic is ", f_stat, " and the p_value is ", p_value))
}
# ______________________ END ________________________

```

Note: For each question, include R code and output pertinent to your answers.

<br> <br>

## 1. Faraway Chapter 7. Exercise 5.

```{r}
data("prostate") 
?prostate
glimpse(prostate)
```

<br>

-   For the prostate data, fit a model with lpsa as the response and the other variables as predictors.

```{r}
summ(
  mod <- lm(lpsa ~ lcavol + age + lbph + svi + lcp + gleason + pgg45, 
            prostate))
```

<br>

## 1.A. Compute and comment on Kappa and the condition numbers.

```         
-   We compute the conditional number kappa, which measures therelative sizes of the eigenvalues where kappa \> 30 is considered large. Our kappa value of 240 is 8 times more larger than 30, but can only tells us that at least one of the eigenvalue is small relative to the rest. Therefore, we examine other conditional indices eta'j because they indicate whether more than just one independent linear combination is to blame. We see that 5 of 7 conditional indices are above 30, suggesting several independent linear combinations are present in this model.
```

```{r}
kappa_fun(mod)
```

<br>

## 1.B. Compute and comment on the correlations between the predictors. Round to 3 decimal places.

```         
-   In the correlation matrix contour plot below we take note on the strong positive association between gleason and pgg45, between lcp and lcavol, and between svi and lcp
```

```{r, fig.width=6, fig.height=7}
testRes = cor.mtest(prostate |> select(-lpsa, -lweight), conf.level = 0.95)

corrplot(cor(prostate |> select(-lpsa, -lweight)), 
         p.mat = testRes$p, method = 'circle', type = 'lower', insig='blank',
         order = 'AOE', diag = FALSE)$corrPos -> p1
text(p1$x, p1$y, round(p1$corr, 3))
```

<br>

## 1.C. Compute the variance inflation factors. Comment on whether any appear problematic and why.

-   The variance inflation factor (VIF) assesses multicollinearity, which is the ratio of the variance of $\hat{\beta}_j$: when fitting the full model divided by the variance of $\hat{\beta}_j$ if fit on its own. The smallest possible value is 1 indicating absence of collinearity. A rule of thumb of a VIF of over 5 is an indication of a problematic amount of collinearity. VIF is expressed as:

$$
VIF_j = \frac{1}{1=R^2_j} 
$$

-   We see below that our VIF for each variable is below 5 and conclude that just we saw in our correlation plot above, we are not seeing evidence for multicollinearity.

```{r}
ols_vif_tol(mod) |> select(Variables, VIF)
```

<br> <br>

## 2. Faraway Chapter 8. Exercise 4.

<br>

-   For the cars dataset, fit a linear model with distance as the response and speed as the predictor.

```{r}
data("cars")
?cars
glimpse(cars)
summ(mod_cars <- lm(dist ~ speed, cars))
```

<br>

## Test the homoscedasticity assumption using both a scatter plot between the residuals and fitted values and an F-test of equal variance below and above the fitted value of 30. What do you conclude about whether the assumption is met?

-   We can see from the residual plot below that as fitted values increase, variation in the residuals widens such as like a cone shape. We can also see that lower fitted values have residuals higher than 0, while some mid point values are below 0. Therefore, we are seeing evidence for heteroscadasticity.

```{r}
ols_plot_resid_fit(mod_cars, print_plot = FALSE)  +
  theme_nice()
```

-   We examine statistically equal variances for fitted values greater than 30 and values less than or equal to 30. The F-test for equal variances suggest that the variances in the residuals are not equal based on the confidence intervals (95%[1.53, 9.42]) does not contain 0, meaning that we reject the null that the true ration of variances =1 in favor of the alternative hypothesis of unequal variances, according to our F-test. We conclude that we have evidence for non-constant variance both graphically and statistically.

```{r}
var.test(resid(mod_cars)[fitted(mod_cars)>30],
         resid(mod_cars)[fitted(mod_cars)<=30])

```

<br>

## 2.B. Report the estimate of the heteroscedastic consistent variance for the regression slope.

-   The heteroscedastic consistent variance for the slope is .183.

```{r}
hetvar <- mod_cars |> 
  # calculate Heteroscedasticity-consistent estimation of the covariance matrix for coefficients
  vcovHC()  |>  
  # gives variances as they are the diagonal of the covariance matrix
  diag() 

hetvar
```

<br>

## 2.C. Construct 95% confidence interval of the regression slope assuming homoscedasticity and using the results in 2.B. How do they compare?

```{r}
coef(mod_cars)[2]
```

```{r}
confint(mod_cars)[2,] 
```

```{r}
mod_cars$coefficients[2]+c(-1,1)*qt(0.975,mod_cars$df.residual)*sqrt(hetvar[2])

```

<br>

## 2.D. Check for the lack of fit of the model.

-   We use the function we created at the begining of this rmarkdown and pass it two models, the orignal model and one where we convert the continous variable into a factor.  


```{r}
plot(cars$speed,cars$dist,pch=18,col="blue",cex=.7,
     main="title description") +
  abline(lm(dist~speed, cars),col="red")

summ(mod_cars_a<-lm(resid(mod_cars) ~ factor(speed), cars))
```

-   There is little evidence for lack of fit in this data or model. We failed to reject the null hypothesis, p \>.05, which might be a limitation of the data. We took note in the $R^2$ of these models and report that for mod_cars (original model) the $R^2$ is .65, while the $R^2$ for mod_cars_a (lack of fit model) it dropped to .40, suggesting that the data may not support these many levels in the factor considering that there are only 50 observations and 25 factor levels. In this case, we may not want to rely on $R^2$ as much.

```{r}
f_stat_lack_fit_fun(mod_cars, mod_cars_a)
```

-   We can also get the f-statistic from the `olsrr::ols_pure_error()`

```{r}
## use the olsrr package to conduct a lack of fit test
ols_pure_error_anova(mod_cars)
```

